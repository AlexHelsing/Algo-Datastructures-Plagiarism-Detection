Plagiarism detection
====================

Time spent per group member:
* Stefan Ingimarsson 12hrs
* Henrik: 11 hrs
* Alexander Helsing: 11hrs

Task 1: Analyzing the slow program
----------------------------------

**Question**
What is the asymptotic complexity of findSimilarity?
Answer in terms of N, the total number of 5-grams in the input files.
Assume that the number of duplicate occurrences of 5-grams is a small constant - that is, there is not much plagiarised text.
Explain briefly.

The complexity for buildindex should be O(1) and findsimilairity should be o(n^2 * k^2).

findsimilairity: the outer loop iterates over each ngram object in the index bst. since index is a bst,
it has o(log n) time complexity, where n is the number of unique ngram. with that in mind, outer loop is
o(n log n). the inner loops iterate over path object that is associated with the ngram. worst case, all
documents share the same ngram, so the inner two loops execute for every pair of path obj in the index.
this is k*(k-1)/2 iterations for each ngrams. since there are n unique ngram obj in the index, the total
number of iterations is k*(k-1)*n/2. the innermost loop a pathpair obj is created and used as key in
similarity. containskey, get and put methods have o(log m) where m is the number of key-value pairs in bst.
similarity can have at most n*(n-1)/2 key value pairs(one for each unique pair of path objects). the complexity
is o(n^2 log n). overall comlexity is o(n^2 log n). in d*k terms, where n=d*k, we can write it as: o(n^2 log n)=
o((dk)^2 log(dk)) = o(d^2 k^2 log(dk)).

TODO

**Question**
How long did the program take on the 'small' and 'medium' directories?
Is the ratio between the times what you would expect,
given the asymptotic complexity? Explain very briefly why.

Give the path to the document set: documents/small
Reading all input files took 0,06 seconds.
Building n-gram index took 0,00 seconds.
Computing similarity scores took 2,74 seconds.
Finding the most similar files took 0,01 seconds.
In total the program took 2,80 seconds.

And:

Reading all input files took 0,20 seconds.
Building n-gram index took 0,00 seconds.
Computing similarity scores took 565,77 seconds.
Finding the most similar files took 0,02 seconds.
In total the program took 565,99 seconds.

Since the complexity is quadratic and the medium file is 10 times the size of the small file (10^^=100), we are
a bit surprised that it took that much longer.


TODO

**Question**
How long do you predict the program would take to run on
the 'huge' directory? Show your calculations.

Huge directory = 4 million N. Medium directory = 200k N, 20 times less. 20^2 = 400 so 566*400 = 226 400 sec,
which is 3773 minutes, which is about 63 hours.

TODO

Task 2: Using an index
----------------------

**Question**
Which of the three BSTs in the program usually become unbalanced?
Say very briefly:
- how you observed this,
- why you think this is the case.

Nonbalancing BSTs usually become unbalanced when it comes to the larger directories.
A nonbalancing tree is not designed to balance itself automatically. This means that
if the data being inserted into the tree is not added in a balanced manner, the tree
can become unbalanced.

TODO

**Question** (optional)
Is there a simple way to stop these trees becoming unbalanced?

TODO (optional)

Task 3: Using scapegoat trees instead of BSTs
---------------------------------------------

For the below questions, we denote by N the total number of 5-grams.
We assume there is a (small) constant number of duplicate occurrences of 5-grams.

**Question**
What is the asymptotic complexity of buildIndex?
Justify briefly.
O(nk log(nk)), where n is the number of path objects in the files bst and k is average number of ngram obj
per ngram array in the files. the innermost loop iterates over ngram obj in each ngram for each path in files.
so the innermost loop affects the complexity the most. since the index bst can have at most n*k key-value pairs,
the time complexity of the inner loop is o(nk log(nk)). the outer loop is o(n log n) where n is the number of path
obj in the files bst, since the bst has o(log n) time. therefore the complexity is o(nk log(nk)).

TODO

**Question**
What is the asymptotic complexity of findSimilarity?
Justify briefly.
O(n log n) where n is the total number of ngrams in the document set.

TODO

**Question** (optional)
Instead of the previous assumption, we now allow an arbitrary total similarity score S.
Express the asymptotic complexity of the two functions in terms of both N and S (at the same time).

TODO (optional)

Appendix: general information
=============================

**Question**
Do you know of any bugs or limitations?

TODO

**Question**
Did you collaborate with any other students on this lab?
If so, write with whom and in what way you collaborated.
Also list any resources (including the web) you have used in creating your design.

The readme was great and we read through that a lot. Also used the openDSA a lot, especially for understanding the trees.
Otherwise there was a lot of googles searches and youtube videos regarding n-grams and scapegoat trees.
Received some help from previous CS student, but as in lab 2 it was tough to understand for him as well, but he could
help us wrap our brain around some concepts.

TODO

**Question**
Did you encounter any serious problems?

Not really, no.

TODO

**Question**
What is your feedback on this assignment?
-It was pretty confusing at first with ngrams and what we actually needed to do in task 2.
However once we understood everything, things got a little easier.
-Extremely challenging but very informative. It has given several 'aha' moments when one
 finally understands a concept.

TODO

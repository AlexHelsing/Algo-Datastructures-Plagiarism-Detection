Plagiarism detection
====================

Time spent per group member:
* Stefan Ingimarsson 12hrs
* Henrik: 11 hrs
* Alexander Helsing: 11hrs

Task 1: Analyzing the slow program
----------------------------------

**Question**
What is the asymptotic complexity of findSimilarity?
Answer in terms of N, the total number of 5-grams in the input files.
Assume that the number of duplicate occurrences of 5-grams is a small constant - that is, there is not much plagiarised text.
Explain briefly.

The complexity for buildindex should be O(1) and findsimilairity should be O(Nlog N).

findsimilairity: n is the number of files and k is the length of ngrams. there are two nested loops
iterating all over the files and within those two nested loops there are two more nested loops.

buildindex: it just creates an empty bst and returns without doing much else.

TODO

**Question**
How long did the program take on the 'small' and 'medium' directories?
Is the ratio between the times what you would expect,
given the asymptotic complexity? Explain very briefly why.

Give the path to the document set: documents/small
Reading all input files took 0,06 seconds.
Building n-gram index took 0,00 seconds.
Computing similarity scores took 2,74 seconds.
Finding the most similar files took 0,01 seconds.
In total the program took 2,80 seconds.

And:

Reading all input files took 0,20 seconds.
Building n-gram index took 0,00 seconds.
Computing similarity scores took 565,77 seconds.
Finding the most similar files took 0,02 seconds.
In total the program took 565,99 seconds.

Since the complexity is quadratic and the medium file is 10 times the size of the small file (10^^=100), we are
a bit surprised that it took that much longer.


TODO

**Question**
How long do you predict the program would take to run on
the 'huge' directory? Show your calculations.

Huge directory = 4 million N. Medium directory = 200k N, 20 times less. 20^2 = 400 so 566*400 = 226 400 sec,
which is 3773 minutes, which is about 63 hours.

TODO

Task 2: Using an index
----------------------

**Question**
Which of the three BSTs in the program usually become unbalanced?
Say very briefly:
- how you observed this,
- why you think this is the case.

The most unbalanced tree is likely to be the one that has the most nodes added or removed from it.
The index BST is the most unbalanced because it has the most nodes added to it; one node for each n-gram in each file.
To see which BST is the most unbalanced, you can print out the height of each tree. The higher the tree, the more unbalanced the BST.


TODO

**Question** (optional)
Is there a simple way to stop these trees becoming unbalanced?

TODO (optional)

Task 3: Using scapegoat trees instead of BSTs
---------------------------------------------

For the below questions, we denote by N the total number of 5-grams.
We assume there is a (small) constant number of duplicate occurrences of 5-grams.

**Question**
What is the asymptotic complexity of buildIndex?
Justify briefly.
O(n*m) where n is the number of documents and m is the number of 5grams per document. the outer loop
iterates over n and the inner loop iterates over m.

TODO

**Question**
What is the asymptotic complexity of findSimilarity?
Justify briefly.
In the worst case scenario, the outer loop traverses through all n-grams in the index, which has a time complexity of O(N log N).
For every n-gram, the inner loop examines all pairs of files that hold the specific n-gram.




TODO

**Question** (optional)
Instead of the previous assumption, we now allow an arbitrary total similarity score S.
Express the asymptotic complexity of the two functions in terms of both N and S (at the same time).

TODO (optional)

Appendix: general information
=============================

**Question**
Do you know of any bugs or limitations?

TODO

**Question**
Did you collaborate with any other students on this lab?
If so, write with whom and in what way you collaborated.
Also list any resources (including the web) you have used in creating your design.

The readme was great and we read through that a lot. Also used the openDSA a lot, especially for understanding the trees.
Otherwise there was a lot of googles searches and youtube videos regarding n-grams and scapegoat trees.
Received some help from previous CS student, but as in lab 2 it was tough to understand for him as well, but he could
help us wrap our brain around some concepts.

TODO

**Question**
Did you encounter any serious problems?

Not really, no.

TODO

**Question**
What is your feedback on this assignment?
-It was pretty confusing at first with ngrams and what we actually needed to do in task 2.
However once we understood everything, things got a little easier.
-Extremely challenging but very informative. It has given several 'aha' moments when one
 finally understands a concept.

TODO

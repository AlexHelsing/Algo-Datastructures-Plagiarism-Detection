Plagiarism detection
====================

Time spent per group member:
* Stefan Ingimarsson 12hrs
* Henrik: 11 hrs
* Alexander Helsing: 11hrs

Task 1: Analyzing the slow program
----------------------------------

**Question**
What is the asymptotic complexity of findSimilarity?
Answer in terms of N, the total number of 5-grams in the input files.
Assume that the number of duplicate occurrences of 5-grams is a small constant - that is, there is not much plagiarised text.
Explain briefly.

in the original findsimialrity, before we modified it; we start by looping through each path in the first input
binary tree, denoted as files. we do this with two paths, path1 and path2. we check if the paths are the same, in that case we
skip them. this should have complexity o(n).

we then get the arrays of ngrams for both paths and loop through them, one array after the other. looping
through one array should take o(n), and since we have two arrays that task should be o(n^2).

we then compare the ngrams in the arrays and create a pathpair object with the two paths. if the pathpair is
not already in the bst we add it. adding elements to bst is o(1). all together we get o(n^3).

TODO

**Question**
How long did the program take on the 'small' and 'medium' directories?
Is the ratio between the times what you would expect,
given the asymptotic complexity? Explain very briefly why.

Give the path to the document set: documents/small
Reading all input files took 0,06 seconds.
Building n-gram index took 0,00 seconds.
Computing similarity scores took 2,74 seconds.
Finding the most similar files took 0,01 seconds.
In total the program took 2,80 seconds.

And:

Reading all input files took 0,20 seconds.
Building n-gram index took 0,00 seconds.
Computing similarity scores took 565,77 seconds.
Finding the most similar files took 0,02 seconds.
In total the program took 565,99 seconds.

Since the complexity is quadratic and the medium file is 10 times the size of the small file (10^^=100), we are
a bit surprised that it took that much longer.


TODO

**Question**
How long do you predict the program would take to run on
the 'huge' directory? Show your calculations.

Huge directory = 4 million N. Medium directory = 200k N, 20 times less. 20^2 = 400 so 566*400 = 226 400 sec,
which is 3773 minutes, which is about 63 hours.

TODO

Task 2: Using an index
----------------------

**Question**
Which of the three BSTs in the program usually become unbalanced?
Say very briefly:
- how you observed this,
- why you think this is the case.

Nonbalancing BSTs usually become unbalanced when it comes to the larger directories.
A nonbalancing tree is not designed to balance itself automatically. This means that
if the data being inserted into the tree is not added in a balanced manner, the tree
can become unbalanced. Files usually become unbalanced as the addition of new nodes
skews the height of the tree. If new nodes are inserted in a sorted order, it can
result in a skewed tree where all the nodes are in one direction.

TODO

**Question** (optional)
Is there a simple way to stop these trees becoming unbalanced?

TODO (optional)

Task 3: Using scapegoat trees instead of BSTs
---------------------------------------------

For the below questions, we denote by N the total number of 5-grams.
We assume there is a (small) constant number of duplicate occurrences of 5-grams.

**Question**
What is the asymptotic complexity of buildIndex?
Justify briefly.
o(n log n). we start by iterating through the ngrams in the paths. we check if the ngrams contain the keys.
if they do not contain the key, we add it to an array, and add the array to the bst. if they contain the key,
we add it directly to the bst.

TODO

**Question**
What is the asymptotic complexity of findSimilarity?
Justify briefly.
O(n log n). the outer loop iterates through the ngrams, which has complexity o(n).
and the next loops compare the ngram in path1 to the ngram in path2 which has complexity o(log n).
these together should have the complexity o(n log n).

TODO

**Question** (optional)
Instead of the previous assumption, we now allow an arbitrary total similarity score S.
Express the asymptotic complexity of the two functions in terms of both N and S (at the same time).

TODO (optional)

Appendix: general information
=============================

**Question**
Do you know of any bugs or limitations?

TODO

**Question**
Did you collaborate with any other students on this lab?
If so, write with whom and in what way you collaborated.
Also list any resources (including the web) you have used in creating your design.

The readme was great and we read through that a lot. Also used the openDSA a lot, especially for understanding the trees.
Otherwise there was a lot of googles searches and youtube videos regarding n-grams and scapegoat trees.
Received some help from previous CS student, but as in lab 2 it was tough to understand for him as well, but he could
help us wrap our brain around some concepts.

TODO

**Question**
Did you encounter any serious problems?

Not really, no.

TODO

**Question**
What is your feedback on this assignment?
-It was pretty confusing at first with ngrams and what we actually needed to do in task 2.
However once we understood everything, things got a little easier.
-Extremely challenging but very informative. It has given several 'aha' moments when one
 finally understands a concept.

TODO
